This will remove the default chunk size for each priority level.

Do not yet use, this is a work-in-progress, and if you do not specify explicit chunk sizes, you will suffer a significant drop in performance.

diff --git a/nonblock/BackgroundWrite.py b/nonblock/BackgroundWrite.py
index 3e812b4..7fd3e8e 100644
--- a/nonblock/BackgroundWrite.py
+++ b/nonblock/BackgroundWrite.py
@@ -27,8 +27,9 @@ __all__ = ('BackgroundWriteProcess', 'BackgroundIOPriority', 'bgwrite', 'bgwrite
 #if DEBUG:
 #    import sys
 
+_SIZE_MEG = 1024 * 1024
 
-def bgwrite(fileObj, data, closeWhenFinished=False, chainAfter=None, ioPrio=4):
+def bgwrite(fileObj, data, closeWhenFinished=False, chainAfter=None, ioPrio=4, chunkSize=_SIZE_MEG * .5):
     '''
         bgwrite - Start a background writing process
 
@@ -47,7 +48,7 @@ def bgwrite(fileObj, data, closeWhenFinished=False, chainAfter=None, ioPrio=4):
             @return - BackgroundWriteProcess - An object representing the state of this operation. @see BackgroundWriteProcess
     '''
 
-    thread = BackgroundWriteProcess(fileObj, data, closeWhenFinished, chainAfter, ioPrio)
+    thread = BackgroundWriteProcess(fileObj, data, closeWhenFinished, chainAfter, ioPrio, chunkSize)
     thread.start()
 
     return thread
@@ -66,9 +67,9 @@ def bgwrite_chunk(fileObj, data, chunkSize, closeWhenFinished=False, chainAfter=
 
         @param chunkSize <integer> - The max siZe of each chunk.
     '''
-    chunks = chunk_data(data, chunkSize)
+#    chunks = chunk_data(data, chunkSize)
 
-    return bgwrite(fileObj, chunks, closeWhenFinished, chainAfter, ioPrio)
+    return bgwrite(fileObj, data, closeWhenFinished, chainAfter, ioPrio, chunkSize)
 
 
 class BackgroundIOPriority(object):
@@ -78,9 +79,9 @@ class BackgroundIOPriority(object):
             See __init__ for fields
     '''
 
-    __slots__ = ('chainPollTime', 'defaultChunkSize', 'bandwidthPct', 'numChunksRateSmoothing')
+    __slots__ = ('chainPollTime', 'bandwidthPct', 'numChunksRateSmoothing')
 
-    def __init__(self, chainPollTime, defaultChunkSize, bandwidthPct, numChunksRateSmoothing=5):
+    def __init__(self, chainPollTime, bandwidthPct, numChunksRateSmoothing=5):
         '''
             __init__ - Create a BackgroundIOPriority.
 
@@ -90,12 +91,6 @@ class BackgroundIOPriority(object):
             @param chainPollTime - float > 0, When chaining, this is the sleep time between checking if prior is finished.
                 Too low and the polling takes up CPU time, too high and you'll lose a little time in between chained writes, while gaining interactivity elsewhere.
 
-            @param defaultChunkSize - integer > 0, When providing a straight string/bytes to bgwrite (instead of chunking yourself, or using bgwrite_chunk) this will
-                be used as the max size of each chunk. Each chunk is written and a flush is issued (if the stream supports it).
-                Increasing this increases throughput while decreasing interactivity
-
-            @param bandwidthPct - integer > 0 and < 100. This is the percentage of overall bandwidth that this task will attempt to use.
-
               A high number means higher throughput at the cost of lest interactivity for other tasks, a low number means the opposite.
 
               So, for example, a bandwidthPct of "50" will attempt to use "50%" of the available bandwidth. Note, this does not represent theroetical
@@ -108,7 +103,7 @@ class BackgroundIOPriority(object):
               See #bandwidthPct for the other half of the story. The higher this number, the more "fair" your application will be against a constant
               rate of I/O by other applications, but the less able it may be to play fair when the external I/O is spiking.
 
-              Also, consider that this is related to the #defaultChunkSize, as it is not a constant period of time. The default of "5" should be okay,
+              Also, consider that this is related to the #chunkSize, as it is not a constant period of time. The default of "5" should be okay,
               but you may want to tune it if you use really large or really small chunk sizes.
 
 
@@ -117,7 +112,6 @@ class BackgroundIOPriority(object):
 
 
         self.chainPollTime = chainPollTime
-        self.defaultChunkSize = defaultChunkSize
         self.bandwidthPct = float(bandwidthPct)
         if bandwidthPct <= 0 or bandwidthPct > 100:
             raise ValueError('Given bandwidthPct %f must be > 0 and <= 100')
@@ -139,16 +133,16 @@ _SIZE_MEG = 1024 * 1024
 
 # BG_IO_PRIOS - Predefined I/O priorities, 1-10. The lower the number, the more throughput at the cost of interactivity
 BG_IO_PRIOS = {
-    1  : BackgroundIOPriority(.0009, _SIZE_MEG * 5,    100), # Maximum throughput, no regard for interactivity.
-    2  : BackgroundIOPriority(.0009, _SIZE_MEG * 4,     90),
-    3  : BackgroundIOPriority(.0015, _SIZE_MEG * 3,     78),
-    4  : BackgroundIOPriority(.0015, _SIZE_MEG * 2,     72),
-    5  : BackgroundIOPriority(.0019, _SIZE_MEG * 1.6,   65),
-    6  : BackgroundIOPriority(.0019, _SIZE_MEG * .75,   55),
-    7  : BackgroundIOPriority(.0024, _SIZE_MEG * .69,   45),
-    8  : BackgroundIOPriority(.0024, _SIZE_MEG * .5,    35),
-    9  : BackgroundIOPriority(.0031, _SIZE_MEG * .3,    30),
-    10 : BackgroundIOPriority(.0100, _SIZE_MEG * .25,   20), # Least throughput, most interactivity, very little throughput
+    1  : BackgroundIOPriority(.0009,  100), # Maximum throughput, no regard for interactivity.
+    2  : BackgroundIOPriority(.0009,   90),
+    3  : BackgroundIOPriority(.0015,   78),
+    4  : BackgroundIOPriority(.0015,   72),
+    5  : BackgroundIOPriority(.0019,   65),
+    6  : BackgroundIOPriority(.0019,   55),
+    7  : BackgroundIOPriority(.0024,   45),
+    8  : BackgroundIOPriority(.0024,   35),
+    9  : BackgroundIOPriority(.0031,   30),
+    10 : BackgroundIOPriority(.0100,   20), # Least throughput, most interactivity, very little throughput
 }
 
 
@@ -166,7 +160,7 @@ class BackgroundWriteProcess(threading.Thread):
     '''
 # Design question: What about errors?
 
-    def __init__(self, fileObj, dataBlocks, closeWhenFinished=False, chainAfter=None, ioPrio=4):
+    def __init__(self, fileObj, dataBlocks, closeWhenFinished=False, chainAfter=None, ioPrio=4, chunkSize=_SIZE_MEG * .5):
         '''
             __init__ - Create the BackgroundWriteProcess thread. You should probably use bgwrite or bgwrite_chunk instead of calling this directly.
 
@@ -180,6 +174,12 @@ class BackgroundWriteProcess(threading.Thread):
 
             @param ioPrio <int/BackgroundIOPriority> - If an integer (1-10), a predefined BackgroundIOPriority will be used. 1 is highest throughput, 10 is most interactivity. You can also pass in your own BackgroundIOPriority object if you want to define a custom profile.
 
+            @param chunkSize - integer > 0, When providing a straight string/bytes to bgwrite (instead of chunking yourself, or using bgwrite_chunk) this will
+                be used as the max size of each chunk. Each chunk is written and a flush is issued (if the stream supports it).
+                Increasing this increases throughput while decreasing interactivity
+
+            @param bandwidthPct - integer > 0 and < 100. This is the percentage of overall bandwidth that this task will attempt to use.
+
 
             @raises ValueError - If ioPrio is neither a BackgroundIOPriority nor integer 1-10 inclusive
                                - If chainAfter is not a BackgroundWriteProcess or None
@@ -196,7 +196,8 @@ class BackgroundWriteProcess(threading.Thread):
                 raise ValueError('Invalid ioPrio: %s. Available priority levels are: %s' %(str(ioPrio), str(list(BG_IO_PRIOS.keys()))) )
 
         if type(dataBlocks) not in (list, tuple):
-            dataBlocks = chunk_data(dataBlocks, self.backgroundIOPriority.defaultChunkSize)
+            dataBlocks = chunk_data(dataBlocks, chunkSize)
+
         self.remainingData = deque(dataBlocks)
 
         self.closeWhenFinished = closeWhenFinished
